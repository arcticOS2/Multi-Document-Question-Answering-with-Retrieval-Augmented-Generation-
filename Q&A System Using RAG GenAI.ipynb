{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7754205,
          "sourceType": "datasetVersion",
          "datasetId": 4534007
        },
        {
          "sourceId": 7754251,
          "sourceType": "datasetVersion",
          "datasetId": 4534036
        },
        {
          "sourceId": 7754281,
          "sourceType": "datasetVersion",
          "datasetId": 4534053
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 42px; color : #393e46 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #00adb5; border-radius: 5px 5px;\"><strong>MultiDocs Q&A With RAG</strong></p>"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-04T03:10:28.693404Z",
          "iopub.execute_input": "2024-03-04T03:10:28.693916Z",
          "iopub.status.idle": "2024-03-04T03:10:44.577641Z",
          "shell.execute_reply.started": "2024-03-04T03:10:28.693859Z",
          "shell.execute_reply": "2024-03-04T03:10:44.57632Z"
        },
        "id": "TNkgpjEi9XKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS';\"><strong>Objective :</strong></p>\n",
        "<p style = \"font-size : 17px; color : #810000 ; font-family : 'Comic Sans MS'; \">The primary objective of this Kaggle notebook is to design, implement, and demonstrate a sophisticated question-answering system utilizing Retrieval-Augmented Generation (RAG) technology. This system will be capable of ingesting multiple documents as its knowledge base, understanding the context and nuances within these documents, and generating precise, informative answers to a wide range of user queries. By leveraging the RAG framework, the project aims to highlight the system's ability to perform real-time information retrieval from a diverse document set, fuse this information seamlessly, and produce answers that are not only accurate but also contextually enriched.</p>"
      ],
      "metadata": {
        "id": "ggnYkbY59XKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS';\"><strong>What is RAG ?</strong></p>\n",
        "\n",
        "<ul>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \">RAG is a technique for augmenting LLM knowledge with additional data.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \">LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "i_eQ1zv-9XKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS';\"><strong>RAG Architecture</strong></p>\n",
        "<p style = \"font-size : 15px; color : #810000; font-family : 'Comic Sans MS';\"><strong>A typical RAG application has two main components: </strong></p>\n",
        "\n",
        "<ol>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Indexing:</strong> a pipeline for ingesting data from a source and indexing it. This usually happens offline.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Retrieval and generation:</strong> The actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
        "</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "W_uqcAxA9XKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS';\"><strong>Indexing:</strong></p>\n",
        "<ol>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Load:</strong> First we need to load our data. This is done with DocumentLoaders.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Split:</strong> Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window.</li>\n",
        "<li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Store:</strong> We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "OzOv_lum9XKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS';\"><strong>Retrieval and Generation</strong></p>\n",
        "<ol>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Retrieve:</strong> Given a user input, relevant splits are retrieved from storage using a Retriever.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>Generate:</strong> A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data.</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "Cw2Htg629XKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '0'></a>\n",
        "<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : #f9b208; border-radius: 5px 5px;\"><strong>Table of Contents</strong></p>\n",
        "\n",
        "* [Indexing](#1.0)\n",
        "    * [Data Loading](#1.1)\n",
        "    * [Data Extraction](#1.2)\n",
        "    * [Chunking](#1.3)\n",
        "    * [Embeddings Creation](#1.4)\n",
        "    * [Indexing](#1.5)\n",
        "    \n",
        "    \n",
        "* [Retrieval and Generation](#2.0)\n",
        "    * [Retriever](#2.1)\n",
        "    * [LLM Model](#2.2)\n",
        "\n",
        "\n",
        "* [Results](#3.0)\n",
        "* [Conclusion](#4.0)"
      ],
      "metadata": {
        "id": "aZZERhtA9XKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Required Libraries\n",
        "%pip install python-docx\n",
        "%pip install python-pptx\n",
        "%pip install PyPDF2\n",
        "%pip install langchain\n",
        "%pip install langchain_community\n",
        "%pip install langchain_google_genai\n",
        "%pip install langchain_text_splitters\n",
        "%pip install sentence-transformers\n",
        "%pip install faiss-cpu\n",
        "%pip install cohere"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-05T02:50:29.712398Z",
          "iopub.execute_input": "2024-03-05T02:50:29.712831Z",
          "iopub.status.idle": "2024-03-05T02:52:30.337192Z",
          "shell.execute_reply.started": "2024-03-05T02:50:29.712795Z",
          "shell.execute_reply": "2024-03-05T02:52:30.336121Z"
        },
        "trusted": true,
        "id": "WNzSl5fN9XKE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary Imports\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "from pptx import Presentation\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts  import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:30.344014Z",
          "iopub.execute_input": "2024-03-05T02:52:30.344343Z",
          "iopub.status.idle": "2024-03-05T02:52:31.786572Z",
          "shell.execute_reply.started": "2024-03-05T02:52:30.344312Z",
          "shell.execute_reply": "2024-03-05T02:52:31.785736Z"
        },
        "trusted": true,
        "id": "MvyfPZUJ9XKF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '1.1'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Data Loading</strong></p>\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \">For This Notebook, I have taken Three different types of data.</p>\n",
        "<ul>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>PDF :-</strong> 10th class History BOOK</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>PPT :-</strong> Project ppt</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>DOCS :- </strong>Project synopsis report.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "-D3vASZB9XKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = open('/kaggle/input/ncert-class-10-history/NCERT-Class-10-History.pdf','rb')\n",
        "ppt_file = Presentation(\"/kaggle/input/mid-report-ppt/Nitesh_PPT.pptx\")\n",
        "doc_file = Document('/kaggle/input/final-report-synopsis/final_project synopsis.docx')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:31.787716Z",
          "iopub.execute_input": "2024-03-05T02:52:31.788221Z",
          "iopub.status.idle": "2024-03-05T02:52:31.846877Z",
          "shell.execute_reply.started": "2024-03-05T02:52:31.788191Z",
          "shell.execute_reply": "2024-03-05T02:52:31.845865Z"
        },
        "trusted": true,
        "id": "srruJDEB9XKF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '1.2'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Data Extraction</strong></p>\n",
        "<ul>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>PDF :-</strong> Pdf data is extracted using PyPDF2 and all text is stored in a string.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>PPT :-</strong> PPT data is extracted using python-pptx module and all text is stored in a string.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \"><strong>DOCS :- </strong>Docs data is extracted using python-docs module and all text is stored in a string.</li>\n",
        "</ul>\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \"> After Extracting all data seperately, I have combined all text in a single string for further text processing.</p>\n",
        "<ul>"
      ],
      "metadata": {
        "id": "b36wJ7zH9XKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting pdf data\n",
        "pdf_text = \"\"\n",
        "pdf_reader = PdfReader(pdf_file)\n",
        "for page in pdf_reader.pages:\n",
        "    pdf_text += page.extract_text()\n",
        "\n",
        "# extracting ppt data\n",
        "ppt_text = \"\"\n",
        "for slide in ppt_file.slides:\n",
        "    for shape in slide.shapes:\n",
        "        if hasattr(shape, \"text\"):\n",
        "            ppt_text += shape.text + '\\n'\n",
        "\n",
        "# extracting doc data\n",
        "doc_text = \"\"\n",
        "for paragraph in doc_file.paragraphs:\n",
        "    doc_text += paragraph.text + '\\n'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:31.849502Z",
          "iopub.execute_input": "2024-03-05T02:52:31.849812Z",
          "iopub.status.idle": "2024-03-05T02:52:33.367234Z",
          "shell.execute_reply.started": "2024-03-05T02:52:31.849785Z",
          "shell.execute_reply": "2024-03-05T02:52:33.366148Z"
        },
        "trusted": true,
        "id": "R71jvTHx9XKF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# merging all the text\n",
        "\n",
        "all_text = pdf_text + '\\n' + ppt_text + '\\n' + doc_text\n",
        "len(all_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:33.368843Z",
          "iopub.execute_input": "2024-03-05T02:52:33.369191Z",
          "iopub.status.idle": "2024-03-05T02:52:33.37725Z",
          "shell.execute_reply.started": "2024-03-05T02:52:33.369162Z",
          "shell.execute_reply": "2024-03-05T02:52:33.376138Z"
        },
        "trusted": true,
        "id": "6Wb8qYDd9XKF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '1.3'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Chunking</strong></p>\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \"> In this step I am creating the chunks of data, for this step I am using Recursive Character Splitter which break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window.</p>"
      ],
      "metadata": {
        "id": "f8-uMJfQ9XKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the text into chunks for embeddings creation\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap = 200, # This is helpul to handle the data loss while chunking.\n",
        "        length_function = len,\n",
        "        separators=['\\n', '\\n\\n', ' ', '']\n",
        "    )\n",
        "\n",
        "chunks = text_splitter.split_text(text = all_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:33.37866Z",
          "iopub.execute_input": "2024-03-05T02:52:33.378959Z",
          "iopub.status.idle": "2024-03-05T02:52:33.401505Z",
          "shell.execute_reply.started": "2024-03-05T02:52:33.378927Z",
          "shell.execute_reply": "2024-03-05T02:52:33.400587Z"
        },
        "trusted": true,
        "id": "474dzXWw9XKG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:33.40275Z",
          "iopub.execute_input": "2024-03-05T02:52:33.403188Z",
          "iopub.status.idle": "2024-03-05T02:52:33.417114Z",
          "shell.execute_reply.started": "2024-03-05T02:52:33.403158Z",
          "shell.execute_reply": "2024-03-05T02:52:33.416121Z"
        },
        "trusted": true,
        "id": "2liKxTRO9XKG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HuggingFaceHub_API_Token']= #HuggingFaceHub_API_Token\n",
        "os.environ['GOOGLE_API_KEY']= #GOOGLE_API_KEY\n",
        "os.environ['cohere_api_key'] = #cohere_api_key"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:33.418475Z",
          "iopub.execute_input": "2024-03-05T02:52:33.419252Z",
          "iopub.status.idle": "2024-03-05T02:52:33.425047Z",
          "shell.execute_reply.started": "2024-03-05T02:52:33.419222Z",
          "shell.execute_reply": "2024-03-05T02:52:33.424252Z"
        },
        "trusted": true,
        "id": "V0aBrlFA9XKG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '1.4'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Embeddings Creation</strong></p>\n",
        "\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \">Embeddings creation is a crucial preprocessing step in the development of document-based Question and Answering (Q&A) systems. This process involves converting textual data from documents and questions into dense, high-dimensional vectors known as embeddings. These embeddings are designed to capture the semantic meaning of words, sentences, or even entire documents, enabling the Q&A system to understand and process natural language more effectively.</p>"
      ],
      "metadata": {
        "id": "e55rIRkA9XKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing embeddings model\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:33.426277Z",
          "iopub.execute_input": "2024-03-05T02:52:33.42657Z",
          "iopub.status.idle": "2024-03-05T02:52:39.653396Z",
          "shell.execute_reply.started": "2024-03-05T02:52:33.426545Z",
          "shell.execute_reply": "2024-03-05T02:52:39.652558Z"
        },
        "trusted": true,
        "id": "X3wmghGS9XKG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '1.5'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Indexing</strong></p>\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \">Indexing data using Facebook AI Similarity Search (FAISS) is a pivotal step in developing efficient and scalable document-based Question and Answering (Q&A) systems. FAISS is a library that facilitates the efficient search for similarities in large datasets, especially useful for tasks involving high-dimensional vectors like text embeddings. When applied to document-based Q&A, FAISS indexes the embeddings of document chunks (e.g., paragraphs, sentences) to optimize the retrieval process.</p>"
      ],
      "metadata": {
        "id": "YzWm96uu9XKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing the data using FAISS\n",
        "vectorstore = FAISS.from_texts(chunks, embedding = embeddings)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:52:39.65553Z",
          "iopub.execute_input": "2024-03-05T02:52:39.656217Z",
          "iopub.status.idle": "2024-03-05T02:53:21.893732Z",
          "shell.execute_reply.started": "2024-03-05T02:52:39.656176Z",
          "shell.execute_reply": "2024-03-05T02:53:21.892582Z"
        },
        "trusted": true,
        "id": "xGib0OBu9XKG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '2.1'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Retriever</strong></p>\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \">In the development of document-based Question and Answering (Q&A) systems, creating a retriever is a crucial step that directly impacts the system's ability to find relevant information efficiently. The retriever utilizes the pre-indexed embeddings of document chunks, searching through them to find the most relevant pieces of content in response to a user query. This process involves setting up a retrieval mechanism that leverages similarity search to identify the best matches for the query embeddings within the indexed data.</p>"
      ],
      "metadata": {
        "id": "QJR9xWX29XKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating retriever\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:53:21.895153Z",
          "iopub.execute_input": "2024-03-05T02:53:21.895496Z",
          "iopub.status.idle": "2024-03-05T02:53:21.899924Z",
          "shell.execute_reply.started": "2024-03-05T02:53:21.895468Z",
          "shell.execute_reply": "2024-03-05T02:53:21.899143Z"
        },
        "trusted": true,
        "id": "Wky1Ue0a9XKG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.invoke(\"How did the Swadeshi Movement influence Indian industries in the early 20th century?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:53:21.901Z",
          "iopub.execute_input": "2024-03-05T02:53:21.901316Z",
          "iopub.status.idle": "2024-03-05T02:53:21.965077Z",
          "shell.execute_reply.started": "2024-03-05T02:53:21.901289Z",
          "shell.execute_reply": "2024-03-05T02:53:21.964148Z"
        },
        "trusted": true,
        "id": "Jg1LXzQ49XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrieved_docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:53:21.966493Z",
          "iopub.execute_input": "2024-03-05T02:53:21.966793Z",
          "iopub.status.idle": "2024-03-05T02:53:21.973551Z",
          "shell.execute_reply.started": "2024-03-05T02:53:21.966767Z",
          "shell.execute_reply": "2024-03-05T02:53:21.972262Z"
        },
        "trusted": true,
        "id": "uEpt7P0o9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:53:21.977833Z",
          "iopub.execute_input": "2024-03-05T02:53:21.97822Z",
          "iopub.status.idle": "2024-03-05T02:53:21.984136Z",
          "shell.execute_reply.started": "2024-03-05T02:53:21.978184Z",
          "shell.execute_reply": "2024-03-05T02:53:21.983165Z"
        },
        "trusted": true,
        "id": "0SvEtXPO9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '2.2'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>LLM Models</strong></p>\n",
        "\n",
        "<ul>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \">Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand, generate, and interact with human language in a way that mimics human-like understanding. These models are trained on vast amounts of text data, allowing them to grasp the nuances of language, including grammar, context, and even cultural references. The capabilities of LLMs extend beyond simple text generation; they can perform a variety of tasks such as translation, summarization, question answering, and even code generation.</li>\n",
        "    <li style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; \">One of the key technologies behind LLMs is the Transformer architecture, which enables the model to pay attention to different parts of the input text differently, thereby understanding the context and relationships between words and phrases more effectively. This architecture has led to significant improvements in natural language processing tasks and is the foundation of many state-of-the-art LLMs.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "G0BSWKma9XKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \">Cohere LLM</p>"
      ],
      "metadata": {
        "id": "ecaIiZCO9XKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
        "                not contained in the context, say \"answer not available in context\" \\n\\n\n",
        "                Context: \\n {context}?\\n\n",
        "                Question: \\n {question} \\n\n",
        "                Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_template)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:56:42.810775Z",
          "iopub.execute_input": "2024-03-05T02:56:42.811165Z",
          "iopub.status.idle": "2024-03-05T02:56:42.816761Z",
          "shell.execute_reply.started": "2024-03-05T02:56:42.811135Z",
          "shell.execute_reply": "2024-03-05T02:56:42.815608Z"
        },
        "trusted": true,
        "id": "u-qZ7BRB9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a single string of relevant documents given by Faiss.\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:56:44.073292Z",
          "iopub.execute_input": "2024-03-05T02:56:44.074357Z",
          "iopub.status.idle": "2024-03-05T02:56:44.07912Z",
          "shell.execute_reply.started": "2024-03-05T02:56:44.07432Z",
          "shell.execute_reply": "2024-03-05T02:56:44.078078Z"
        },
        "trusted": true,
        "id": "adMw0DCm9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Chain\n",
        "\n",
        "def generate_answer(question):\n",
        "    cohere_llm = Cohere(model=\"command\", temperature=0.1, cohere_api_key = os.getenv('cohere_api_key'))\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | cohere_llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain.invoke(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:56:46.858752Z",
          "iopub.execute_input": "2024-03-05T02:56:46.859449Z",
          "iopub.status.idle": "2024-03-05T02:56:46.865488Z",
          "shell.execute_reply.started": "2024-03-05T02:56:46.859402Z",
          "shell.execute_reply": "2024-03-05T02:56:46.864438Z"
        },
        "trusted": true,
        "id": "vg8d7ni79XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '3.0'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Results</strong></p>"
      ],
      "metadata": {
        "id": "FQC7nrgJ9XKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"How did the Swadeshi Movement influence Indian industries in the early 20th century?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:56:48.60332Z",
          "iopub.execute_input": "2024-03-05T02:56:48.603689Z",
          "iopub.status.idle": "2024-03-05T02:56:53.643818Z",
          "shell.execute_reply.started": "2024-03-05T02:56:48.60366Z",
          "shell.execute_reply": "2024-03-05T02:56:53.642553Z"
        },
        "trusted": true,
        "id": "QYJpl8Px9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"Who is virat kohli\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:57:04.067977Z",
          "iopub.execute_input": "2024-03-05T02:57:04.068422Z",
          "iopub.status.idle": "2024-03-05T02:57:05.212357Z",
          "shell.execute_reply.started": "2024-03-05T02:57:04.068389Z",
          "shell.execute_reply": "2024-03-05T02:57:05.211476Z"
        },
        "trusted": true,
        "id": "MPxNpFYp9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"How did the East India Company contribute to the opium trade with China in the 19th century?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:57:07.067619Z",
          "iopub.execute_input": "2024-03-05T02:57:07.06834Z",
          "iopub.status.idle": "2024-03-05T02:57:13.058796Z",
          "shell.execute_reply.started": "2024-03-05T02:57:07.068304Z",
          "shell.execute_reply": "2024-03-05T02:57:13.057625Z"
        },
        "trusted": true,
        "id": "srNDZkUs9XKH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"What was the impact of British manufactured goods on the Indian market during the 19th century?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:57:13.060741Z",
          "iopub.execute_input": "2024-03-05T02:57:13.061167Z",
          "iopub.status.idle": "2024-03-05T02:57:19.307761Z",
          "shell.execute_reply.started": "2024-03-05T02:57:13.06113Z",
          "shell.execute_reply": "2024-03-05T02:57:19.306745Z"
        },
        "trusted": true,
        "id": "VS2xEyxx9XKP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"What is the primary goal of the project?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:57:19.309236Z",
          "iopub.execute_input": "2024-03-05T02:57:19.309876Z",
          "iopub.status.idle": "2024-03-05T02:57:21.604174Z",
          "shell.execute_reply.started": "2024-03-05T02:57:19.309837Z",
          "shell.execute_reply": "2024-03-05T02:57:21.603339Z"
        },
        "trusted": true,
        "id": "5RJxDDj59XKP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"Which machine learning algorithms are utilized in the project?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:57:21.606968Z",
          "iopub.execute_input": "2024-03-05T02:57:21.6074Z",
          "iopub.status.idle": "2024-03-05T02:57:51.392475Z",
          "shell.execute_reply.started": "2024-03-05T02:57:21.607361Z",
          "shell.execute_reply": "2024-03-05T02:57:51.391375Z"
        },
        "trusted": true,
        "id": "uRn4Fm7b9XKP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"What preprocessing techniques are used in the project?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:57:51.393672Z",
          "iopub.execute_input": "2024-03-05T02:57:51.394383Z",
          "iopub.status.idle": "2024-03-05T02:58:08.774561Z",
          "shell.execute_reply.started": "2024-03-05T02:57:51.394354Z",
          "shell.execute_reply": "2024-03-05T02:58:08.77345Z"
        },
        "trusted": true,
        "id": "7JvPnCt19XKP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ans = generate_answer(\"How was the project deployed?\")\n",
        "print(ans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T02:58:08.775849Z",
          "iopub.execute_input": "2024-03-05T02:58:08.776289Z",
          "iopub.status.idle": "2024-03-05T02:58:12.2708Z",
          "shell.execute_reply.started": "2024-03-05T02:58:08.776249Z",
          "shell.execute_reply": "2024-03-05T02:58:12.269756Z"
        },
        "trusted": true,
        "id": "FITZRQID9XKQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = '4.0'></a>\n",
        "<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Conclusion</strong></p>\n",
        "<p style = \"font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' \">In conclusion, this Kaggle notebook has successfully demonstrated the application of Retrieval-Augmented Generation (RAG) for multi-document Question and Answering. It showcased the power of combining retrieval and generation capabilities to provide accurate, context-aware answers sourced from multiple documents. Through detailed examples, performance evaluations, and interactive demonstrations, the notebook highlights the efficiency and scalability of RAG in handling complex Q&A tasks.</p>\n"
      ],
      "metadata": {
        "id": "StexJ__B9XKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"font-size : 13px; color : #810000 ; font-family : 'Comic Sans MS' \">\n",
        "If you found this helpful an upvote would be very much appreciated :-)</p>"
      ],
      "metadata": {
        "id": "UCdredAN9XKQ"
      }
    }
  ]
}